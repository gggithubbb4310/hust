{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import logging\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch_num = 30\n",
    "mini_batch_size = 10\n",
    "lambda1 = 100\n",
    "lambda2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        super(SegmentationDataset, self).__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        # 仅选择有效的图像文件\n",
    "        self.images = [f for f in sorted(os.listdir(images_dir)) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        self.masks = [f for f in sorted(os.listdir(masks_dir)) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        \n",
    "        #self.images = sorted(os.listdir(images_dir))\n",
    "        #self.masks = sorted(os.listdir(masks_dir))\n",
    "        assert len(self.images) == len(self.masks), \"图像和掩码数量不匹配\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "\n",
    "        # 读取图像\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"无法读取图像文件：{img_path}\")\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.expand_dims(image, axis=0)  # [1, H, W]\n",
    "\n",
    "        # 读取掩码\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"无法读取掩码文件：{mask_path}\")\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        mask = np.expand_dims(mask, axis=0)  # [1, H, W]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1_CAN8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1_CAN8, self).__init__()\n",
    "        chn = 64\n",
    "        self.leakyrelu1 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu3 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu4 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu5 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu6 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu7 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu8 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # 编码器部分\n",
    "        self.g1_conv1 = nn.Conv2d(1,     chn,   3, dilation=1, padding=1)\n",
    "        self.g1_conv2 = nn.Conv2d(chn,   chn,   3, dilation=1, padding=1)\n",
    "        self.g1_conv3 = nn.Conv2d(chn,   chn*2, 3, dilation=2, padding=2)\n",
    "        self.g1_conv4 = nn.Conv2d(chn*2, chn*4, 3, dilation=4, padding=4)\n",
    "        self.g1_conv5 = nn.Conv2d(chn*4, chn*8, 3, dilation=8, padding=8)\n",
    "        \n",
    "        # 解码器部分\n",
    "        self.g1_conv6 = nn.Conv2d(chn*8, chn*4, 3, dilation=4, padding=4)\n",
    "        self.g1_conv7 = nn.Conv2d(chn*4, chn*2, 3, dilation=2, padding=2)\n",
    "        self.g1_conv8 = nn.Conv2d(chn*2, chn,   3, dilation=1, padding=1)\n",
    "        self.g1_conv9 = nn.Conv2d(chn,   1,     1, dilation=1)\n",
    "        \n",
    "        # 批归一化层\n",
    "        self.g1_bn1 = nn.BatchNorm2d(chn)\n",
    "        self.g1_bn2 = nn.BatchNorm2d(chn)\n",
    "        self.g1_bn3 = nn.BatchNorm2d(chn*2)\n",
    "        self.g1_bn4 = nn.BatchNorm2d(chn*4)\n",
    "        self.g1_bn5 = nn.BatchNorm2d(chn*8)\n",
    "        self.g1_bn6 = nn.BatchNorm2d(chn*4)\n",
    "        self.g1_bn7 = nn.BatchNorm2d(chn*2)\n",
    "        self.g1_bn8 = nn.BatchNorm2d(chn)\n",
    "    \n",
    "    def forward(self, input_images):  # 输入[B, 1, 128, 128], 输出[B, 1, 128, 128]\n",
    "        # 编码器部分\n",
    "        net = self.g1_conv1(input_images)\n",
    "        net = self.g1_bn1(net)\n",
    "        net = self.leakyrelu1(net)\n",
    "        \n",
    "        net = self.g1_conv2(net)\n",
    "        net = self.g1_bn2(net)\n",
    "        net = self.leakyrelu2(net)\n",
    "        \n",
    "        net = self.g1_conv3(net)\n",
    "        net = self.g1_bn3(net)\n",
    "        net = self.leakyrelu3(net)\n",
    "        \n",
    "        net = self.g1_conv4(net)\n",
    "        net = self.g1_bn4(net)\n",
    "        net = self.leakyrelu4(net)\n",
    "        \n",
    "        net = self.g1_conv5(net)\n",
    "        net = self.g1_bn5(net)\n",
    "        net = self.leakyrelu5(net)\n",
    "        \n",
    "        # 解码器部分\n",
    "        net = self.g1_conv6(net)\n",
    "        net = self.g1_bn6(net)\n",
    "        net = self.leakyrelu6(net)\n",
    "        \n",
    "        net = self.g1_conv7(net)\n",
    "        net = self.g1_bn7(net)\n",
    "        net = self.leakyrelu7(net)\n",
    "        \n",
    "        net = self.g1_conv8(net)\n",
    "        net = self.g1_bn8(net)\n",
    "        net = self.leakyrelu8(net)\n",
    "        \n",
    "        # 最后一层卷积\n",
    "        output = self.g1_conv9(net)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2_UCAN64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2_UCAN64, self).__init__()\n",
    "        chn = 64\n",
    "        self.leakyrelu1 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu3 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu4 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu5 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu6 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu7 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu8 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu9 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu10 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu11 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu12 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu13 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # 编码器部分\n",
    "        self.g2_conv1 = nn.Conv2d(1,   chn, 3, dilation=1, padding=1)\n",
    "        self.g2_conv2 = nn.Conv2d(chn, chn, 3, dilation=2, padding=2)\n",
    "        self.g2_conv3 = nn.Conv2d(chn, chn, 3, dilation=4, padding=4)\n",
    "        self.g2_conv4 = nn.Conv2d(chn, chn, 3, dilation=8, padding=8)\n",
    "        self.g2_conv5 = nn.Conv2d(chn, chn, 3, dilation=16, padding=16)\n",
    "        self.g2_conv6 = nn.Conv2d(chn, chn, 3, dilation=32, padding=32)\n",
    "        self.g2_conv7 = nn.Conv2d(chn, chn, 3, dilation=64, padding=64)\n",
    "        self.g2_conv8 = nn.Conv2d(chn, chn, 3, dilation=32, padding=32)\n",
    "        \n",
    "        # 解码器部分\n",
    "        self.g2_conv9 = nn.Conv2d(chn*2, chn, 3, dilation=16, padding=16)\n",
    "        self.g2_conv10 = nn.Conv2d(chn*2, chn, 3, dilation=8, padding=8)\n",
    "        self.g2_conv11 = nn.Conv2d(chn*2, chn, 3, dilation=4, padding=4)\n",
    "        self.g2_conv12 = nn.Conv2d(chn*2, chn, 3, dilation=2, padding=2)\n",
    "        self.g2_conv13 = nn.Conv2d(chn*2, chn, 3, dilation=1, padding=1)\n",
    "        self.g2_conv14 = nn.Conv2d(chn, 1,   1, dilation=1)\n",
    "        \n",
    "        # 批归一化层\n",
    "        self.g2_bn1 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn2 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn3 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn4 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn5 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn6 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn7 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn8 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn9 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn10 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn11 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn12 = nn.BatchNorm2d(chn)\n",
    "        self.g2_bn13 = nn.BatchNorm2d(chn)\n",
    "    \n",
    "    def forward(self, input_images):  # 输入[B, 1, 128, 128], 输出[B, 1, 128, 128]\n",
    "        # 编码器部分\n",
    "        net1 = self.g2_conv1(input_images)\n",
    "        net1 = self.g2_bn1(net1)\n",
    "        net1 = self.leakyrelu1(net1)\n",
    "        \n",
    "        net2 = self.g2_conv2(net1)\n",
    "        net2 = self.g2_bn2(net2)\n",
    "        net2 = self.leakyrelu2(net2)\n",
    "        \n",
    "        net3 = self.g2_conv3(net2)\n",
    "        net3 = self.g2_bn3(net3)\n",
    "        net3 = self.leakyrelu3(net3)\n",
    "        \n",
    "        net4 = self.g2_conv4(net3)\n",
    "        net4 = self.g2_bn4(net4)\n",
    "        net4 = self.leakyrelu4(net4)\n",
    "        \n",
    "        net5 = self.g2_conv5(net4)\n",
    "        net5 = self.g2_bn5(net5)\n",
    "        net5 = self.leakyrelu5(net5)\n",
    "        \n",
    "        net6 = self.g2_conv6(net5)\n",
    "        net6 = self.g2_bn6(net6)\n",
    "        net6 = self.leakyrelu6(net6)\n",
    "        \n",
    "        net7 = self.g2_conv7(net6)\n",
    "        net7 = self.g2_bn7(net7)\n",
    "        net7 = self.leakyrelu7(net7)\n",
    "        \n",
    "        net8 = self.g2_conv8(net7)\n",
    "        net8 = self.g2_bn8(net8)\n",
    "        net8 = self.leakyrelu8(net8)\n",
    "        \n",
    "        # 拼接 net6 和 net8\n",
    "        net9 = torch.cat([net6, net8], dim=1)  # [B, 128, 32, 32]\n",
    "        \n",
    "        net9 = self.g2_conv9(net9)\n",
    "        net9 = self.g2_bn9(net9)\n",
    "        net9 = self.leakyrelu9(net9)\n",
    "        \n",
    "        # 拼接 net5 和 net9\n",
    "        net10 = torch.cat([net5, net9], dim=1)  # [B, 128, 32, 32]\n",
    "        \n",
    "        net10 = self.g2_conv10(net10)\n",
    "        net10 = self.g2_bn10(net10)\n",
    "        net10 = self.leakyrelu10(net10)\n",
    "        \n",
    "        # 拼接 net4 和 net10\n",
    "        net11 = torch.cat([net4, net10], dim=1)  # [B, 128, 32, 32]\n",
    "        \n",
    "        net11 = self.g2_conv11(net11)\n",
    "        net11 = self.g2_bn11(net11)\n",
    "        net11 = self.leakyrelu11(net11)\n",
    "        \n",
    "        # 拼接 net3 和 net11\n",
    "        net12 = torch.cat([net3, net11], dim=1)  # [B, 128, 32, 32]\n",
    "        \n",
    "        net12 = self.g2_conv12(net12)\n",
    "        net12 = self.g2_bn12(net12)\n",
    "        net12 = self.leakyrelu12(net12)\n",
    "        \n",
    "        # 拼接 net2 和 net12\n",
    "        net13 = torch.cat([net2, net12], dim=1)  # [B, 128, 32, 32]\n",
    "        \n",
    "        net13 = self.g2_conv13(net13)\n",
    "        net13 = self.g2_bn13(net13)\n",
    "        net13 = self.leakyrelu13(net13)\n",
    "        \n",
    "        # 最后一层卷积\n",
    "        net14 = self.g2_conv14(net13)\n",
    "        \n",
    "        return net14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.leakyrelu1 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu3 = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu4 = nn.LeakyReLU(0.2)\n",
    "        self.Tanh1 = nn.Tanh()\n",
    "        self.Tanh2 = nn.Tanh()\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # 卷积层\n",
    "        self.d_conv1 = nn.Conv2d(2,  24, 3, dilation=1, padding=1)\n",
    "        self.d_conv2 = nn.Conv2d(24, 24, 3, dilation=1, padding=1)\n",
    "        self.d_conv3 = nn.Conv2d(24, 24, 3, dilation=1, padding=1)\n",
    "        self.d_conv4 = nn.Conv2d(24, 1,  3, dilation=1, padding=1)\n",
    "        \n",
    "        # 批归一化层\n",
    "        self.d_bn1 = nn.BatchNorm2d(24)\n",
    "        self.d_bn2 = nn.BatchNorm2d(24)\n",
    "        self.d_bn3 = nn.BatchNorm2d(24)\n",
    "        self.d_bn4 = nn.BatchNorm2d(1)\n",
    "        self.d_bn5 = nn.BatchNorm2d(128)\n",
    "        self.d_bn6 = nn.BatchNorm2d(64)\n",
    "        self.d_bn7 = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "    \n",
    "    def forward(self, input_images):  # 输入[3B, 2, 128, 128], 输出[B, 1, 128, 128]\n",
    "        # 最大池化层，减少空间维度\n",
    "        net = F.max_pool2d(input_images, kernel_size=[2,2])  # [3B, 2, 64, 64]\n",
    "        net = F.max_pool2d(net, kernel_size=[2,2])           # [3B, 2, 32, 32]\n",
    "        \n",
    "        # 卷积层和激活函数\n",
    "        net = self.d_conv1(net)\n",
    "        net = self.d_bn1(net)\n",
    "        net = self.leakyrelu1(net)\n",
    "        \n",
    "        net = self.d_conv2(net)\n",
    "        net = self.d_bn2(net)\n",
    "        net = self.leakyrelu2(net)\n",
    "        \n",
    "        net = self.d_conv3(net)\n",
    "        net = self.d_bn3(net)\n",
    "        net = self.leakyrelu3(net)\n",
    "        \n",
    "        net = self.d_conv4(net)\n",
    "        net = self.d_bn4(net)\n",
    "        net1 = self.leakyrelu4(net)  # [3B, 1, 32, 32]\n",
    "        \n",
    "        # 展平操作\n",
    "        net = net1.view(-1, 1024)  # [3B, 1024]\n",
    "        \n",
    "        # 全连接层1\n",
    "        net = self.fc1(net)         # [3B, 128]\n",
    "        net = net.unsqueeze(2).unsqueeze(3)  # [3B, 128, 1, 1]\n",
    "        net = self.d_bn5(net)\n",
    "        net = self.Tanh1(net)       # [3B, 128, 1, 1]\n",
    "        \n",
    "        # 展平操作\n",
    "        net = net.view(-1, 128)     # [3B, 128]\n",
    "        \n",
    "        # 全连接层2\n",
    "        net = self.fc2(net)         # [3B, 64]\n",
    "        net = net.unsqueeze(2).unsqueeze(3)  # [3B, 64, 1, 1]\n",
    "        net = self.d_bn6(net)\n",
    "        net = self.Tanh2(net)       # [3B, 64, 1, 1]\n",
    "        \n",
    "        # 展平操作\n",
    "        net = net.view(-1, 64)      # [3B, 64]\n",
    "        \n",
    "        # 全连接层3\n",
    "        net = self.fc3(net)         # [3B, 3]\n",
    "        net = net.unsqueeze(2).unsqueeze(3)  # [3B, 3, 1, 1]\n",
    "        net = self.d_bn7(net)\n",
    "        net = self.Softmax(net)     # [3B, 3, 1, 1]\n",
    "        net = net.squeeze(3).squeeze(2)      # [3B, 3]\n",
    "        \n",
    "        # 分割输出\n",
    "        realscore0, realscore1, realscore2 = torch.split(net, mini_batch_size, dim=0)\n",
    "        feat0, feat1, feat2 = torch.split(net1, mini_batch_size, dim=0)\n",
    "        \n",
    "        # 特征距离\n",
    "        featDist = torch.mean(torch.pow(feat1 - feat2, 2))\n",
    "    \n",
    "        return realscore0, realscore1, realscore2, featDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(log_file):\n",
    "    logger = logging.getLogger('TrainingLogger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # 创建文件处理器\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # 创建控制台处理器\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, optimizer, epoch, it):\n",
    "    return {\n",
    "        'epoch': epoch,\n",
    "        'it': it,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1Measure(pred, target, threshold=0.5):\n",
    "    pred_binary = (pred > threshold).astype(np.float32)\n",
    "    target_binary = target.astype(np.float32)\n",
    "    TP = np.sum((pred_binary == 1) & (target_binary == 1))\n",
    "    FP = np.sum((pred_binary == 1) & (target_binary == 0))\n",
    "    FN = np.sum((pred_binary == 0) & (target_binary == 1))\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 保存输出的总路径\n",
    "    root_result_dir = os.path.join('pytorch_outputs', 'results')\n",
    "    os.makedirs(root_result_dir, exist_ok=True)\n",
    "    metrics_dir = os.path.join(root_result_dir, 'metrics')\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    #os.makedirs(os.path.join(root_result_dir, 'models'), exist_ok=True)\n",
    "\n",
    "    # 当前时间，日志文件的后缀\n",
    "    time_suffix = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime(time.time()))\n",
    "    # 日志文件\n",
    "    log_file = os.path.join(root_result_dir, f'log_train_g1g2_{time_suffix}.txt')\n",
    "    logger = create_logger(log_file)\n",
    "    # 定义dataset\n",
    "    trainsplit = SegmentationDataset(\n",
    "        images_dir='/root/autodl-fs/data/train/image',\n",
    "        masks_dir='/root/autodl-fs/data/train/mask',\n",
    "    )\n",
    "    trainset = DataLoader(\n",
    "        trainsplit,\n",
    "        batch_size=mini_batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=4,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    testsplit = SegmentationDataset(\n",
    "        images_dir='/root/autodl-fs/data/MDvsFA_test/image',\n",
    "        masks_dir='/root/autodl-fs/data/MDvsFA_test/mask',\n",
    "    )\n",
    "    testset_MDvsFA = DataLoader(\n",
    "        testsplit,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    testsplit = SegmentationDataset(\n",
    "        images_dir='/root/autodl-fs/data/SIRST_test/image',\n",
    "        masks_dir='/root/autodl-fs/data/SIRST_test/mask',\n",
    "    )\n",
    "    testset_Sirst = DataLoader(\n",
    "        testsplit,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # 定义3个Model\n",
    "    g1 = Generator1_CAN8().cuda()\n",
    "    g2 = Generator2_UCAN64().cuda()\n",
    "    dis = Discriminator().cuda()\n",
    "\n",
    "    # 定义3个优化器\n",
    "    optimizer_g1 = optim.Adam(g1.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    optimizer_g2 = optim.Adam(g2.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(dis.parameters(), lr=1e-5, betas=(0.5, 0.999))\n",
    "\n",
    "    # 定义loss\n",
    "    loss1 = nn.BCEWithLogitsLoss()\n",
    "    it = 0\n",
    "    for epoch in range(0, max_epoch_num):\n",
    "        # 调整学习率\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            for p in optimizer_g1.param_groups:\n",
    "                p['lr'] *= 0.2\n",
    "            for q in optimizer_g2.param_groups:\n",
    "                q['lr'] *= 0.2\n",
    "            for r in optimizer_d.param_groups:\n",
    "                r['lr'] *= 0.2\n",
    "            logger.info(f\"Adjusted learning rates at epoch {epoch + 1}\")\n",
    "\n",
    "        # 训练一个周期\n",
    "        logger.info(f'Now we are training epoch {epoch + 1}!')\n",
    "        total_it_per_epoch = len(trainset)\n",
    "        for bt_idx, data in enumerate(trainset):\n",
    "            # 训练一个batch\n",
    "            torch.cuda.empty_cache()  # 释放之前占用的缓存\n",
    "            it += 1\n",
    "\n",
    "            #  训练判别器\n",
    "            dis.train() \n",
    "            g1.eval()\n",
    "            g2.eval()\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # 将输入输出放到cuda上\n",
    "                input_images, output_images = data  # [B, 1, 128, 128], [B, 1, 128, 128]\n",
    "                input_images = input_images.cuda(non_blocking=True).float()\n",
    "                output_images = output_images.cuda(non_blocking=True).float()\n",
    "                \n",
    "                # 生成器生成假图像\n",
    "                g1_out = g1(input_images)  # [B, 1, 128, 128]\n",
    "                g1_out = torch.clamp(g1_out, 0.0, 1.0)\n",
    "                \n",
    "                g2_out = g2(input_images)  # [B, 1, 128, 128]\n",
    "                g2_out = torch.clamp(g2_out, 0.0, 1.0)\n",
    "\n",
    "            # 准备判别器输入\n",
    "            pos1 = torch.cat([input_images, 2 * output_images - 1], dim=1)  # [B, 2, 128, 128]\n",
    "            neg1 = torch.cat([input_images, 2 * g1_out - 1], dim=1)          # [B, 2, 128, 128]  \n",
    "            neg2 = torch.cat([input_images, 2 * g2_out - 1], dim=1)          # [B, 2, 128, 128] \n",
    "            disc_input  = torch.cat([pos1, neg1, neg2], dim=0)              # [3*B, 2, 128, 128]\n",
    "\n",
    "            # 判别器前向传播\n",
    "            logits_real, logits_fake1, logits_fake2, Lgc = dis(disc_input)  # [B, 3], [B, 3], [B, 3], [1]\n",
    "\n",
    "            # 真实标签为1，假标签为0\n",
    "            gen_gt  = torch.cat([\n",
    "                torch.ones(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float()\n",
    "            ], dim=1)\n",
    "            gen_gt1 = torch.cat([\n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.ones(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float()\n",
    "            ], dim=1)\n",
    "            gen_gt2 = torch.cat([\n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.zeros(mini_batch_size, 1).cuda(non_blocking=True).float(), \n",
    "                torch.ones(mini_batch_size, 1).cuda(non_blocking=True).float()\n",
    "            ], dim=1)\n",
    "\n",
    "            # 计算损失\n",
    "            ES0 = torch.mean(loss1(logits_real, gen_gt))\n",
    "            ES1 = torch.mean(loss1(logits_fake1, gen_gt1))\n",
    "            ES2 = torch.mean(loss1(logits_fake2, gen_gt2))\n",
    "            disc_loss = ES0 + ES1 + ES2\n",
    "            #logger.info(f\"Discriminator loss: {disc_loss.item()}\")\n",
    "            disc_loss.backward() # 误差反向传播\n",
    "            optimizer_d.step() # 更新参数\n",
    "\n",
    "            #  训练生成器 G1\n",
    "            dis.eval() \n",
    "            g1.train()\n",
    "            g2.eval()\n",
    "            optimizer_g1.zero_grad()\n",
    "\n",
    "            # 生成器1生成假图像\n",
    "            g1_out = g1(input_images)  # [B, 1, 128, 128]\n",
    "            g1_out = torch.clamp(g1_out, 0.0, 1.0)\n",
    "\n",
    "            # 计算损失\n",
    "            MD1 = torch.mean(torch.mul(torch.pow(g1_out - output_images, 2), output_images))\n",
    "            FA1 = torch.mean(torch.mul(torch.pow(g1_out - output_images, 2), 1 - output_images))\n",
    "            MF_loss1 = lambda1 * MD1 + FA1 \n",
    "\n",
    "            # 准备判别器输入\n",
    "            pos1 = torch.cat([input_images, 2 * output_images - 1], dim=1)  # [B, 2, 128, 128]\n",
    "            neg1 = torch.cat([input_images, 2 * g1_out - 1], dim=1)          # [B, 2, 128, 128]  \n",
    "            neg2 = torch.cat([input_images, 2 * g2_out - 1], dim=1)          # [B, 2, 128, 128] \n",
    "            disc_input  = torch.cat([pos1, neg1, neg2], dim=0)              # [3*B, 2, 128, 128]\n",
    "\n",
    "            # 判别器前向传播\n",
    "            with torch.no_grad():\n",
    "                logits_real, logits_fake1, logits_fake2, Lgc = dis(disc_input)  # [B, 3], [B, 3], [B, 3], [1]\n",
    "\n",
    "            # 生成器1的损失\n",
    "            gen_adv_loss1 = torch.mean(loss1(logits_fake1, gen_gt))\n",
    "            gen_loss1  = lambda1 * MF_loss1 + lambda2 * gen_adv_loss1 + Lgc\n",
    "            #logger.info(f\"Generator1 loss: {gen_loss1.item()}\")\n",
    "\n",
    "            # 反向传播和优化\n",
    "            gen_loss1.backward()\n",
    "            optimizer_g1.step()\n",
    "\n",
    "            #  训练生成器 G2\n",
    "            dis.eval() \n",
    "            g1.eval()\n",
    "            g2.train()\n",
    "            optimizer_g2.zero_grad()\n",
    "\n",
    "            # 生成器2生成假图像\n",
    "            g2_out = g2(input_images)  # [B, 1, 128, 128]\n",
    "            g2_out = torch.clamp(g2_out, 0.0, 1.0)\n",
    "\n",
    "            # 计算损失\n",
    "            MD2 = torch.mean(torch.mul(torch.pow(g2_out - output_images, 2), output_images))\n",
    "            FA2 = torch.mean(torch.mul(torch.pow(g2_out - output_images, 2), 1 - output_images))\n",
    "            MF_loss2 = MD2 + lambda2 * FA2\n",
    "\n",
    "            # 准备判别器输入\n",
    "            pos1 = torch.cat([input_images, 2 * output_images - 1], dim=1)  # [B, 2, 128, 128]\n",
    "            neg1 = torch.cat([input_images, 2 * g1_out - 1], dim=1)          # [B, 2, 128, 128]  \n",
    "            neg2 = torch.cat([input_images, 2 * g2_out - 1], dim=1)          # [B, 2, 128, 128] \n",
    "            disc_input  = torch.cat([pos1, neg1, neg2], dim=0)              # [3*B, 2, 128, 128]\n",
    "\n",
    "            # 判别器前向传播\n",
    "            with torch.no_grad():\n",
    "                logits_real, logits_fake1, logits_fake2, Lgc = dis(disc_input)  # [B, 3], [B, 3], [B, 3], [1]\n",
    "\n",
    "            # 生成器2的损失\n",
    "            gen_adv_loss2 = torch.mean(loss1(logits_fake2, gen_gt))\n",
    "            gen_loss2  = lambda1 * MF_loss2 + lambda2 * gen_adv_loss2 + Lgc\n",
    "            #logger.info(f\"Generator2 loss: {gen_loss2.item()}\")\n",
    "\n",
    "            # 反向传播和优化\n",
    "            gen_loss2.backward()\n",
    "            optimizer_g2.step()\n",
    "\n",
    "            val_loss_g1_list = []\n",
    "            val_false_ratio_g1_list = []\n",
    "            val_detect_ratio_g1_list = []\n",
    "            val_F1_g1_list = []\n",
    "\n",
    "            val_loss_g2_list = []\n",
    "            val_false_ratio_g2_list = []\n",
    "            val_detect_ratio_g2_list = []\n",
    "            val_F1_g2_list = []\n",
    "\n",
    "            val_loss_g3_list = []\n",
    "            val_false_ratio_g3_list = []\n",
    "            val_detect_ratio_g3_list = []\n",
    "            val_F1_g3_list = []\n",
    "\n",
    "            val_loss_g1_list_sirst = []\n",
    "            val_false_ratio_g1_list_sirst = []\n",
    "            val_detect_ratio_g1_list_sirst = []\n",
    "            val_F1_g1_list_sirst = []\n",
    "\n",
    "            val_loss_g2_list_sirst = []\n",
    "            val_false_ratio_g2_list_sirst = []\n",
    "            val_detect_ratio_g2_list_sirst = []\n",
    "            val_F1_g2_list_sirst = []\n",
    "\n",
    "            val_loss_g3_list_sirst = []\n",
    "            val_false_ratio_g3_list_sirst = []\n",
    "            val_detect_ratio_g3_list_sirst = []\n",
    "            val_F1_g3_list_sirst = []\n",
    "            \n",
    "            # 每100个批次进行一次测试\n",
    "            if (bt_idx + 1) % 100 == 0:\n",
    "                # 在测试集上测试\n",
    "                sum_val_loss_g1 = 0\n",
    "                sum_val_false_ratio_g1 = 0 \n",
    "                sum_val_detect_ratio_g1 = 0\n",
    "                sum_val_F1_g1 = 0\n",
    "                #g1_time = 0\n",
    "                \n",
    "                sum_val_loss_g2 = 0\n",
    "                sum_val_false_ratio_g2 = 0 \n",
    "                sum_val_detect_ratio_g2 = 0\n",
    "                sum_val_F1_g2 = 0\n",
    "                #g2_time = 0\n",
    "                \n",
    "                sum_val_loss_g3 = 0\n",
    "                sum_val_false_ratio_g3 = 0 \n",
    "                sum_val_detect_ratio_g3 = 0\n",
    "                sum_val_F1_g3 = 0\n",
    "\n",
    "                #testset_MDvsFA测试集\n",
    "                for bt_idx_test, data in enumerate(testset_MDvsFA):\n",
    "                    dis.eval() \n",
    "                    g1.eval()\n",
    "                    g2.eval()\n",
    "                    optimizer_g1.zero_grad()\n",
    "                    optimizer_g2.zero_grad()\n",
    "                    optimizer_d.zero_grad()\n",
    "                    # 将输入输出放到cuda上\n",
    "                    input_images, output_images = data  # [1, 1, 128, 128], [1, 1, 128, 128]\n",
    "                    input_images = input_images.cuda(non_blocking=True).float()\n",
    "                    output_images = output_images.cuda(non_blocking=True).float()\n",
    "                    # 生成器1输出\n",
    "                    #stime = time.time()\n",
    "                    g1_out = g1(input_images)  # [1, 1, 128, 128]\n",
    "                    #etime = time.time()\n",
    "                    #g1_time += etime - stime\n",
    "                    #logger.info(f'testing {bt_idx_test}, g1 time is {etime - stime}')\n",
    "                    g1_out = torch.clamp(g1_out, 0.0, 1.0)\n",
    "                    # 生成器2输出\n",
    "                    #stime = time.time()\n",
    "                    g2_out = g2(input_images)  # [1, 1, 128, 128]\n",
    "                    #etime = time.time()\n",
    "                    #g2_time += etime - stime\n",
    "                    #logger.info(f'testing {bt_idx_test}, g2 time is {etime - stime}')\n",
    "                    g2_out = torch.clamp(g2_out, 0.0, 1.0)\n",
    "                    # 生成器融合输出\n",
    "                    g3_out = (g1_out + g2_out) / 2  # 取均值的方式进行融合\n",
    "                    # 转换为numpy\n",
    "                    output_images_np = output_images.cpu().numpy()\n",
    "                    g1_out_np = g1_out.detach().cpu().numpy()\n",
    "                    g2_out_np = g2_out.detach().cpu().numpy()\n",
    "                    g3_out_np = g3_out.detach().cpu().numpy()\n",
    "                    # 计算损失和指标\n",
    "                    val_loss_g1 = np.mean(np.square(g1_out_np - output_images_np))\n",
    "                    sum_val_loss_g1 += val_loss_g1\n",
    "                    val_false_ratio_g1 = np.mean(np.maximum(0, g1_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g1 += val_false_ratio_g1\n",
    "                    val_detect_ratio_g1 = np.sum(g1_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g1 += val_detect_ratio_g1\n",
    "                    val_F1_g1 = calculateF1Measure(g1_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g1 += val_F1_g1\n",
    "                    # 计算生成器2的指标\n",
    "                    val_loss_g2 = np.mean(np.square(g2_out_np - output_images_np))\n",
    "                    sum_val_loss_g2 += val_loss_g2\n",
    "                    val_false_ratio_g2 = np.mean(np.maximum(0, g2_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g2 += val_false_ratio_g2\n",
    "                    val_detect_ratio_g2 = np.sum(g2_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g2 += val_detect_ratio_g2\n",
    "                    val_F1_g2 = calculateF1Measure(g2_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g2 += val_F1_g2\n",
    "                    # 计算融合生成器的指标\n",
    "                    val_loss_g3 = np.mean(np.square(g3_out_np - output_images_np))\n",
    "                    sum_val_loss_g3 += val_loss_g3\n",
    "                    val_false_ratio_g3 = np.mean(np.maximum(0, g3_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g3 += val_false_ratio_g3\n",
    "                    val_detect_ratio_g3 = np.sum(g3_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g3 += val_detect_ratio_g3\n",
    "                    val_F1_g3 = calculateF1Measure(g3_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g3 += val_F1_g3\n",
    "\n",
    "                # 记录每100个批次的结果\n",
    "                testset_MDvsFA_len=len(testset_MDvsFA)\n",
    "                \n",
    "                val_loss_g1_list.append(sum_val_loss_g1 / testset_MDvsFA_len)\n",
    "                val_false_ratio_g1_list.append(sum_val_false_ratio_g1 / testset_MDvsFA_len)\n",
    "                val_detect_ratio_g1_list.append(sum_val_detect_ratio_g1 / testset_MDvsFA_len)\n",
    "                val_F1_g1_list.append(sum_val_F1_g1 / testset_MDvsFA_len)\n",
    "\n",
    "                val_loss_g2_list.append(sum_val_loss_g2 / testset_MDvsFA_len)\n",
    "                val_false_ratio_g2_list.append(sum_val_false_ratio_g2 / testset_MDvsFA_len)\n",
    "                val_detect_ratio_g2_list.append(sum_val_detect_ratio_g2 / testset_MDvsFA_len)\n",
    "                val_F1_g2_list.append(sum_val_F1_g2 / testset_MDvsFA_len)\n",
    "\n",
    "                val_loss_g3_list.append(sum_val_loss_g3 / testset_MDvsFA_len)\n",
    "                val_false_ratio_g3_list.append(sum_val_false_ratio_g3 / testset_MDvsFA_len)\n",
    "                val_detect_ratio_g3_list.append(sum_val_detect_ratio_g3 / testset_MDvsFA_len)\n",
    "                val_F1_g3_list.append(sum_val_F1_g3 / testset_MDvsFA_len)\n",
    "\n",
    "                sum_val_loss_g1_sirst = 0\n",
    "                sum_val_false_ratio_g1_sirst = 0\n",
    "                sum_val_detect_ratio_g1_sirst = 0\n",
    "                sum_val_F1_g1_sirst = 0\n",
    "\n",
    "                sum_val_loss_g2_sirst = 0\n",
    "                sum_val_false_ratio_g2_sirst = 0\n",
    "                sum_val_detect_ratio_g2_sirst = 0\n",
    "                sum_val_F1_g2_sirst = 0\n",
    "\n",
    "                sum_val_loss_g3_sirst = 0\n",
    "                sum_val_false_ratio_g3_sirst = 0\n",
    "                sum_val_detect_ratio_g3_sirst = 0\n",
    "                sum_val_F1_g3_sirst = 0\n",
    "\n",
    "                #testset_Sirst测试集\n",
    "                for bt_idx_test, data in enumerate(testset_Sirst):\n",
    "                    dis.eval() \n",
    "                    g1.eval()\n",
    "                    g2.eval()\n",
    "                    optimizer_g1.zero_grad()\n",
    "                    optimizer_g2.zero_grad()\n",
    "                    optimizer_d.zero_grad()\n",
    "\n",
    "                    input_images, output_images = data  # [1, 1, 128, 128], [1, 1, 128, 128]\n",
    "                    input_images = input_images.cuda(non_blocking=True).float()\n",
    "                    output_images = output_images.cuda(non_blocking=True).float()\n",
    "\n",
    "                    # 生成器1输出\n",
    "                    g1_out = g1(input_images)  \n",
    "                    g1_out = torch.clamp(g1_out, 0.0, 1.0)\n",
    "\n",
    "                    # 生成器2输出\n",
    "                    g2_out = g2(input_images)  \n",
    "                    g2_out = torch.clamp(g2_out, 0.0, 1.0)\n",
    "\n",
    "                    # 生成器融合输出\n",
    "                    g3_out = (g1_out + g2_out) / 2 \n",
    "\n",
    "                    # 转换为numpy\n",
    "                    output_images_np = output_images.cpu().numpy()\n",
    "                    g1_out_np = g1_out.detach().cpu().numpy()\n",
    "                    g2_out_np = g2_out.detach().cpu().numpy()\n",
    "                    g3_out_np = g3_out.detach().cpu().numpy()\n",
    "\n",
    "                    # 计算损失和指标\n",
    "                    val_loss_g1 = np.mean(np.square(g1_out_np - output_images_np))\n",
    "                    sum_val_loss_g1_sirst += val_loss_g1\n",
    "                    val_false_ratio_g1 = np.mean(np.maximum(0, g1_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g1_sirst += val_false_ratio_g1\n",
    "                    val_detect_ratio_g1 = np.sum(g1_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g1_sirst += val_detect_ratio_g1\n",
    "                    val_F1_g1 = calculateF1Measure(g1_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g1_sirst += val_F1_g1\n",
    "\n",
    "                    # 计算生成器2的指标\n",
    "                    val_loss_g2 = np.mean(np.square(g2_out_np - output_images_np))\n",
    "                    sum_val_loss_g2_sirst += val_loss_g2\n",
    "                    val_false_ratio_g2 = np.mean(np.maximum(0, g2_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g2_sirst += val_false_ratio_g2\n",
    "                    val_detect_ratio_g2 = np.sum(g2_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g2_sirst += val_detect_ratio_g2\n",
    "                    val_F1_g2 = calculateF1Measure(g2_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g2_sirst += val_F1_g2\n",
    "\n",
    "                    # 计算融合生成器的指标\n",
    "                    val_loss_g3 = np.mean(np.square(g3_out_np - output_images_np))\n",
    "                    sum_val_loss_g3_sirst += val_loss_g3\n",
    "                    val_false_ratio_g3 = np.mean(np.maximum(0, g3_out_np - output_images_np))\n",
    "                    sum_val_false_ratio_g3_sirst += val_false_ratio_g3\n",
    "                    val_detect_ratio_g3 = np.sum(g3_out_np * output_images_np) / np.maximum(np.sum(output_images_np), 1)\n",
    "                    sum_val_detect_ratio_g3_sirst += val_detect_ratio_g3\n",
    "                    val_F1_g3 = calculateF1Measure(g3_out_np, output_images_np, 0.5)\n",
    "                    sum_val_F1_g3_sirst += val_F1_g3\n",
    "\n",
    "                # 计算testset_Sirst的平均值\n",
    "                testset_Sirst_len = len(testset_Sirst)\n",
    "\n",
    "                val_loss_g1_list_sirst.append(sum_val_loss_g1_sirst / testset_Sirst_len)\n",
    "                val_false_ratio_g1_list_sirst.append(sum_val_false_ratio_g1_sirst / testset_Sirst_len)\n",
    "                val_detect_ratio_g1_list_sirst.append(sum_val_detect_ratio_g1_sirst / testset_Sirst_len)\n",
    "                val_F1_g1_list_sirst.append(sum_val_F1_g1_sirst / testset_Sirst_len)\n",
    "\n",
    "                val_loss_g2_list_sirst.append(sum_val_loss_g2_sirst / testset_Sirst_len)\n",
    "                val_false_ratio_g2_list_sirst.append(sum_val_false_ratio_g2_sirst / testset_Sirst_len)\n",
    "                val_detect_ratio_g2_list_sirst.append(sum_val_detect_ratio_g2_sirst / testset_Sirst_len)\n",
    "                val_F1_g2_list_sirst.append(sum_val_F1_g2_sirst / testset_Sirst_len)\n",
    "\n",
    "                val_loss_g3_list_sirst.append(sum_val_loss_g3_sirst / testset_Sirst_len)\n",
    "                val_false_ratio_g3_list_sirst.append(sum_val_false_ratio_g3_sirst / testset_Sirst_len)\n",
    "                val_detect_ratio_g3_list_sirst.append(sum_val_detect_ratio_g3_sirst / testset_Sirst_len)\n",
    "                val_F1_g3_list_sirst.append(sum_val_F1_g3_sirst / testset_Sirst_len)\n",
    "                \n",
    "                # 记录并打印验证结果\n",
    "                logger.info(\"======================== G1 results ============================\")\n",
    "                avg_val_loss_g1 = sum_val_loss_g1 / testset_MDvsFA_len\n",
    "                avg_val_false_ratio_g1  = sum_val_false_ratio_g1 / testset_MDvsFA_len\n",
    "                avg_val_detect_ratio_g1 = sum_val_detect_ratio_g1 / testset_MDvsFA_len\n",
    "                avg_val_F1_g1 = sum_val_F1_g1 / testset_MDvsFA_len\n",
    "                logger.info(f\"Val L2 Loss G1: {avg_val_loss_g1:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G1: {avg_val_false_ratio_g1:.4f}\")\n",
    "                logger.info(f\"Detection Rate G1: {avg_val_detect_ratio_g1:.4f}\")\n",
    "                logger.info(f\"F1 Measure G1: {avg_val_F1_g1:.4f}\")\n",
    "                #logger.info(f\"G1 Time: {g1_time:.4f} seconds\")\n",
    "                logger.info(\"======================== G2 results ============================\")\n",
    "                avg_val_loss_g2 = sum_val_loss_g2 / testset_MDvsFA_len\n",
    "                avg_val_false_ratio_g2  = sum_val_false_ratio_g2 / testset_MDvsFA_len\n",
    "                avg_val_detect_ratio_g2 = sum_val_detect_ratio_g2 / testset_MDvsFA_len\n",
    "                avg_val_F1_g2 = sum_val_F1_g2 / testset_MDvsFA_len\n",
    "                logger.info(f\"Val L2 Loss G2: {avg_val_loss_g2:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G2: {avg_val_false_ratio_g2:.4f}\")\n",
    "                logger.info(f\"Detection Rate G2: {avg_val_detect_ratio_g2:.4f}\")\n",
    "                logger.info(f\"F1 Measure G2: {avg_val_F1_g2:.4f}\")\n",
    "                #logger.info(f\"G2 Time: {g2_time:.4f} seconds\")\n",
    "                logger.info(\"======================== G3 results ============================\")\n",
    "                avg_val_loss_g3 = sum_val_loss_g3 / testset_MDvsFA_len\n",
    "                avg_val_false_ratio_g3  = sum_val_false_ratio_g3 / testset_MDvsFA_len\n",
    "                avg_val_detect_ratio_g3 = sum_val_detect_ratio_g3 / testset_MDvsFA_len\n",
    "                avg_val_F1_g3 = sum_val_F1_g3 / testset_MDvsFA_len\n",
    "                logger.info(f\"Val L2 Loss G3: {avg_val_loss_g3:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G3: {avg_val_false_ratio_g3:.4f}\")\n",
    "                logger.info(f\"Detection Rate G3: {avg_val_detect_ratio_g3:.4f}\")\n",
    "                logger.info(f\"F1 Measure G3: {avg_val_F1_g3:.4f}\")\n",
    "\n",
    "                # 记录并打印验证结果\n",
    "                logger.info(\"======================== G1 results for testset_Sirst ============================\")\n",
    "                avg_val_loss_g1_sirst = sum_val_loss_g1_sirst / testset_Sirst_len\n",
    "                avg_val_false_ratio_g1_sirst = sum_val_false_ratio_g1_sirst / testset_Sirst_len\n",
    "                avg_val_detect_ratio_g1_sirst = sum_val_detect_ratio_g1_sirst / testset_Sirst_len\n",
    "                avg_val_F1_g1_sirst = sum_val_F1_g1_sirst / testset_Sirst_len\n",
    "                logger.info(f\"Val L2 Loss G1 (Sirst): {avg_val_loss_g1_sirst:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G1 (Sirst): {avg_val_false_ratio_g1_sirst:.4f}\")\n",
    "                logger.info(f\"Detection Rate G1 (Sirst): {avg_val_detect_ratio_g1_sirst:.4f}\")\n",
    "                logger.info(f\"F1 Measure G1 (Sirst): {avg_val_F1_g1_sirst:.4f}\")\n",
    "                #logger.info(f\"G1 Time (Sirst): {g1_time:.4f} seconds\")\n",
    "                \n",
    "                logger.info(\"======================== G2 results for testset_Sirst ============================\")\n",
    "                avg_val_loss_g2_sirst = sum_val_loss_g2_sirst / testset_Sirst_len\n",
    "                avg_val_false_ratio_g2_sirst = sum_val_false_ratio_g2_sirst / testset_Sirst_len\n",
    "                avg_val_detect_ratio_g2_sirst = sum_val_detect_ratio_g2_sirst / testset_Sirst_len\n",
    "                avg_val_F1_g2_sirst = sum_val_F1_g2_sirst / testset_Sirst_len\n",
    "                logger.info(f\"Val L2 Loss G2 (Sirst): {avg_val_loss_g2_sirst:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G2 (Sirst): {avg_val_false_ratio_g2_sirst:.4f}\")\n",
    "                logger.info(f\"Detection Rate G2 (Sirst): {avg_val_detect_ratio_g2_sirst:.4f}\")\n",
    "                logger.info(f\"F1 Measure G2 (Sirst): {avg_val_F1_g2_sirst:.4f}\")\n",
    "                #logger.info(f\"G2 Time (Sirst): {g2_time:.4f} seconds\")\n",
    "\n",
    "                logger.info(\"======================== G3 results for testset_Sirst ============================\")\n",
    "                avg_val_loss_g3_sirst = sum_val_loss_g3_sirst / testset_Sirst_len\n",
    "                avg_val_false_ratio_g3_sirst = sum_val_false_ratio_g3_sirst / testset_Sirst_len\n",
    "                avg_val_detect_ratio_g3_sirst = sum_val_detect_ratio_g3_sirst / testset_Sirst_len\n",
    "                avg_val_F1_g3_sirst = sum_val_F1_g3_sirst / testset_Sirst_len\n",
    "                logger.info(f\"Val L2 Loss G3 (Sirst): {avg_val_loss_g3_sirst:.4f}\")\n",
    "                logger.info(f\"False Alarm Rate G3 (Sirst): {avg_val_false_ratio_g3_sirst:.4f}\")\n",
    "                logger.info(f\"Detection Rate G3 (Sirst): {avg_val_detect_ratio_g3_sirst:.4f}\")\n",
    "                logger.info(f\"F1 Measure G3 (Sirst): {avg_val_F1_g3_sirst:.4f}\")\n",
    "                #logger.info(f\"G3 Time (Sirst): {g3_time:.4f} seconds\")\n",
    "\n",
    "                \n",
    "                # 保存模型检查点\n",
    "                #ckpt_name1 = os.path.join(root_result_dir, 'models', f'g1_epoch_{epoch + 1}_batch_{bt_idx + 1}.pth')\n",
    "                #ckpt_name2 = os.path.join(root_result_dir, 'models', f'g2_epoch_{epoch + 1}_batch_{bt_idx + 1}.pth')\n",
    "                #ckpt_name3 = os.path.join(root_result_dir, 'models', f'dis_epoch_{epoch + 1}_batch_{bt_idx + 1}.pth')\n",
    "                #save_checkpoint(checkpoint_state(g1, optimizer_g1, epoch + 1, it), filename=ckpt_name1)\n",
    "                #save_checkpoint(checkpoint_state(g2, optimizer_g2, epoch + 1, it), filename=ckpt_name2)\n",
    "                #save_checkpoint(checkpoint_state(dis, optimizer_d, epoch + 1, it), filename=ckpt_name3)\n",
    "                #logger.info(f\"Saved models at epoch {epoch + 1}, batch {bt_idx + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
